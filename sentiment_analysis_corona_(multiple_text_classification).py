# -*- coding: utf-8 -*-
"""Sentiment Analysis-Corona (Multible Text Classification).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QPXrP5mpPfEQm21Gd6jJfstk_3GnQRUV
"""

# Nama : Debora Udania Simanjuntak
# Email : debora.udania.simanjuntak@mail.ugm.ac.id
# Kelas : Belajar Pengembangan Machine Learning

# import the libraries and data
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

data_train=pd.read_csv("Corona_NLP_train.csv", delimiter=',',encoding='latin-1')
data_test=pd.read_csv("Corona_NLP_test.csv", delimiter=',',encoding='latin-1')

data=pd.concat([data_train,data_test],axis=0)

# Understanding the Data
data.head()

# Delete the first 4 columns in data

data=data.drop(columns=["UserName","ScreenName","Location","TweetAt"])
data.head()

data.Sentiment.unique()

# The are 5 kind of sentiments in the data, they are Extremely Negative, Negative, Neutral, Extremey Positif and Positive
# Here I am going to group Extremely Negative and Negative into one group and Extremely Positive and Positive into one group as well
data["Sentiment"]=np.where(data["Sentiment"]=="Extremely Positive","Positive",data["Sentiment"])
data["Sentiment"]=np.where(data["Sentiment"]=="Extremely Negative","Negative",data["Sentiment"])

# Text Preprocessing to remove stopwords

import nltk
nltk.download('stopwords')

# Make a function to remove stopwords
from nltk.corpus import stopwords
import re

def remove_stopwords(input_text):
    stopwords_list=stopwords.words("english")
    # Some words which might indicate a certain sentiment are kept via a whitelist
    whitelist = ["n't", "not", "no"]
    words = input_text.split() 
    clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] 
    return " ".join(clean_words)

# Apply function to remove stopwords
data.OriginalTweet=data.OriginalTweet.apply(remove_stopwords)

data.head()

#  Change the string label into numeric form 
category=pd.get_dummies(data.Sentiment)
category
# Concat data and category
data=pd.concat([data,category],axis=1)
# Drop column Sentiment
data=data.drop(columns="Sentiment")
data.head()

# Spilt data into train and test
from sklearn.model_selection import train_test_split as tts

X_data=data.OriginalTweet.values
y_data=data.iloc[:,1:6].values

X_train,X_test,y_train,y_test=tts(X_data,y_data,test_size=0.2,random_state=0)

y_train

# Apply Tokenization so model will understand the data
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer=Tokenizer(num_words=5000,oov_token='-')
tokenizer.fit_on_texts(X_train)
tokenizer.fit_on_texts(X_test)

sequence_train=tokenizer.texts_to_sequences(X_train)
sequence_test=tokenizer.texts_to_sequences(X_test)

padded_train=pad_sequences(sequence_train,maxlen=59)
padded_test=pad_sequences(sequence_test,maxlen=59)

# Build the model

import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Embedding,GlobalAveragePooling1D,Dense,LSTM,Dropout

model=Sequential([Embedding(input_dim=5000,output_dim=16),
                  LSTM(64, recurrent_dropout=0.4),
                  Dropout(0.3),
                  Dense(16, activation='relu'),
                  Dropout(0.3),
                  Dense(3,activation='softmax')])

model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

# Make callback function
import tensorflow as tf
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('val_accuracy')>0.85):
      print("\nAccuracy has reached>85%!")
      self.model.stop_training = True
callbacks = myCallback()

history=model.fit(padded_train,y_train, epochs=50,batch_size=32,
                  validation_data=(padded_test,y_test),verbose=2,callbacks=[callbacks])

plt.plot(history.history.get('accuracy'))
plt.plot(history.history.get('val_accuracy'))
plt.xlabel('epochs')
plt.ylabel('accuracy')
plt.legend(['train','test'])
plt.ylim(0,1)
plt.show()

plt.plot(history.history.get('loss'))
plt.plot(history.history.get('val_loss'))
plt.xlabel('epochs')
plt.ylabel('loss')
plt.legend(['train','test'])
plt.show()

